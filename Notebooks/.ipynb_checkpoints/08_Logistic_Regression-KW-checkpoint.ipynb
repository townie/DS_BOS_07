{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification\n",
    "Logistic Regression is a classification technique that aims to squash the Linear Regression model into the interval [0,1] so that it can be used as a binary classifier.  The resulting score gives an estimate of the probability of an observation belonging to the positive class in a binary classification situation.\n",
    "\n",
    "In this exercise, we'll try out logistic regression against 2 datasets: one for predicting graduate school admission and the other for predicting flight delays.  We'll see how to perform logistic regression with multiple python packages and interpret the subsequent logistic regression results in a probabilistic context. \n",
    "\n",
    "## Learning Goals\n",
    "- Perform Logistic Regression with `statsmodels`\n",
    "- Perform Logistic Regression with `sklearn`\n",
    "- Interpret the results of logistic regression\n",
    "- Handle categorical input (predictor) variables by transforming to dummy variables in `pandas`\n",
    "\n",
    "## Datasets\n",
    "- [Graduate School Admissions](http://www.ats.ucla.edu/stat/data/binary.csv)\n",
    "- [Flight Delays](https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduate School Admission\n",
    "We'll be using the same dataset as [UCLA's Logit Regression in R tutorial](http://www.ats.ucla.edu/stat/r/dae/logit.htm) to explore logistic regression in Python. Our goal will be to identify the various factors that may influence admission into graduate school.\n",
    "\n",
    "The dataset contains several columns which we can use as predictor variables:\n",
    "\n",
    "- gpa\n",
    "- gre score\n",
    "- rank or prestige of an applicant's undergraduate alma mater\n",
    "\n",
    "The fourth column, admit, is our binary target variable. It indicates whether or not a candidate was admitted our not.\n",
    "\n",
    "### Load in the Data\n",
    "Let's use `pandas.read_csv()` to read the dataset into a `DataFrame` for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# read the data in from http://www.ats.ucla.edu/stat/data/binary.csv\n",
    "\n",
    "# take a look at the dataset with head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data\n",
    "Since there is also a `DataFrame` function called `rank()`, let's change the name of the \"rank\" column to \"prestige\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename the 'rank' column because there is also a DataFrame method called 'rank'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize the data with describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a look at the standard deviation of each column with std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice `pandas` function is the [`crosstab()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html) function which allows you to slice across different variables and display frequencies or other aggregations (akin to a [Pivot Table](https://en.wikipedia.org/wiki/Pivot_table) in Excel).  Let's use it to take a look at how the counts of admitted and not admitted look as a function of prestige:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# frequency table cutting prestige and whether or not someone was admitted\n",
    "pd.crosstab(df['admit'], df['prestige'], rownames=['admit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a visual look at this with a bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.crosstab(df['admit'], df['prestige'], rownames=['admit'])\n",
    "x.transpose().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the proportion of admits is higher for the more prestigious undergraduate schools.  As an exercise, try to generate a similar table showing the admits by **gre** score.  First bucket gre score into 7 buckets (200-300, 300-400, 400-500, 500-600, 600-700, 700-800, 800).  \n",
    "\n",
    "**HINT**: Divid the gre column by 100 first, then call a `map()` function on the resulting series where the map makes a call to `math.floor()` (you'll need to import `math` first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice a relationship between gre score and admission here?  Is it what you'd expect?\n",
    "\n",
    "Now let's use the `pandas.DataFrame.hist()` function to generate histograms for each of the 4 variables to get a quick feel for how they're distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot all of the columns with hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dummy Variables\n",
    "`pandas` gives you a great deal of control over how categorical variables are represented. We're going dummify the **prestige** column using [`get_dummies`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html).\n",
    "\n",
    "`get_dummies()` creates a new DataFrame with binary indicator variables for each category/option in the column specified. In this case, prestige has four levels: 1, 2, 3 and 4 (1 being most prestigious). When we call `get_dummies()`, we get a dataframe with four columns, each of which describes one of those levels.\n",
    "\n",
    "Create a new temporary dataframe by calling `get_dummies()` on the prestige column with the  prefix \"prestige\" to create dummy variables for each of the 4 prestige values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummify prestige\n",
    "\n",
    "# Take a look at your dummified frame with head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a clean data frame for the regression\n",
    "cols_to_keep = ['admit', 'gre', 'gpa']\n",
    "data = df[cols_to_keep].join(dummy_prestige.ix[:, 'prestige_2':])\n",
    "print data.head()\n",
    "\n",
    "# manually add the intercept\n",
    "data['intercept'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done, we merge the new dummy columns into the original dataset and get rid of the prestige column which we no longer need.  We're going to treat **prestige_1** as our baseline and exclude it from our fit. This is done to prevent [**multicollinearity**](https://en.wikipedia.org/wiki/Multicollinearity), or the [**dummy variable trap**](https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29#Precautions_in_the_usage_of_dummy_variables) caused by including a dummy variable for every single category.\n",
    "\n",
    "Lastly we're going to add a constant term for our Logistic Regression. The `statsmodels` function we're going to be using requires that intercepts/constants are specified explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the Regression\n",
    "As we did with Linear Regression, first we'll try out the Logistic Regression with `statsmodels`, before showing how we can accomplish the corresponding task with `sklearn`.\n",
    "\n",
    "#### Logistic Regression with `statsmodels`\n",
    "For `statsmodels` we must call `sm.Logit(y, X)` with the target variable **admit** as `y` and the predictor columns as `X`.  We can then call `fit()` to fit our model.  Here's how we might do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select out all of our desired predictors (not admit and prestige_1)\n",
    "train_cols = data.columns[1:]\n",
    "\n",
    "# Create our model\n",
    "logit = sm.Logit(data['admit'], data[train_cols])\n",
    "\n",
    "# fit the model\n",
    "result = logit.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpreting `statsmodels` Results\n",
    "Now that we've fit a model, we can print out some nice summary results as we did before with Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summarize statsmodels results\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're very confident that there is an inverse relationship between the probability of being admitted and the prestige of a candidate's undergraduate school.  We know this because all of their coefficients are (increasingly) negative with very low P values.\n",
    "\n",
    "In other words, the probability of being accepted into a graduate program is higher for students who attended a top ranked undergraduate college (prestige_1==True) as opposed to a lower ranked school with, say, prestige_4==True (remember, a prestige of 1 is the most prestigious and a prestige of 4 is the least prestigious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Odds Ratio\n",
    "Take the exponential of each of the coefficients to generate the odds ratios. This tells you how a 1 unit increase or decrease in a variable affects the odds of being admitted. For example, we can expect the odds of being admitted to decrease by about 50% if the prestige of a school is 2. UCLA gives a more in depth explanation of the odds ratio [here](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print odds ratios only\n",
    "print np.exp(result.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same calculations using the coefficients estimated using the confidence interval to get a better picture for how uncertainty in variables can impact the admission rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# odds ratios and 95% CI\n",
    "params = result.params\n",
    "# Get confidence intervals\n",
    "conf = result.conf_int()\n",
    "# Append the Odds Ratios\n",
    "conf['OR'] = params\n",
    "# Print out the odds ratio 95% confidence intervals\n",
    "conf.columns = ['2.5%', '97.5%', 'OR']\n",
    "print np.exp(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Going Further\n",
    "As a way of evaluating our classifier, we're going to recreate the dataset with every logical combination of input values. This will allow us to see how the predicted probability of admission increases/decreases across different variables. First we're going to generate the combinations using a helper function called `cartesian()` which I originally found [here](http://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays).\n",
    "\n",
    "The other new function we're going to use is `np.linspace()` to create a range of values for \"gre\" and \"gpa\". This creates a range of linearly spaced values from a specified min and maximum value--in our case just the min/max observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define cartesian helper function\n",
    "def cartesian(arrays, out=None):\n",
    "    \"\"\"\n",
    "    Generate a cartesian product of input arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : list of array-like\n",
    "        1-D arrays to form the cartesian product of.\n",
    "    out : ndarray\n",
    "        Array to place the cartesian product in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray\n",
    "        2-D array of shape (M, len(arrays)) containing cartesian products\n",
    "        formed of input arrays.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n",
    "    array([[1, 4, 6],\n",
    "           [1, 4, 7],\n",
    "           [1, 5, 6],\n",
    "           [1, 5, 7],\n",
    "           [2, 4, 6],\n",
    "           [2, 4, 7],\n",
    "           [2, 5, 6],\n",
    "           [2, 5, 7],\n",
    "           [3, 4, 6],\n",
    "           [3, 4, 7],\n",
    "           [3, 5, 6],\n",
    "           [3, 5, 7]])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    arrays = [np.asarray(x) for x in arrays]\n",
    "    dtype = arrays[0].dtype\n",
    "\n",
    "    n = np.prod([x.size for x in arrays])\n",
    "    if out is None:\n",
    "        out = np.zeros([n, len(arrays)], dtype=dtype)\n",
    "\n",
    "    m = n / arrays[0].size\n",
    "    out[:,0] = np.repeat(arrays[0], m)\n",
    "    if arrays[1:]:\n",
    "        cartesian(arrays[1:], out=out[0:m,1:])\n",
    "        for j in xrange(1, arrays[0].size):\n",
    "            out[j*m:(j+1)*m,1:] = out[0:m,1:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instead of generating all possible values of GRE and GPA, we're going\n",
    "# to use an evenly spaced range of 10 values from the min to the max \n",
    "gres = np.linspace(data['gre'].min(), data['gre'].max(), 10)\n",
    "print gres\n",
    "\n",
    "gpas = np.linspace(data['gpa'].min(), data['gpa'].max(), 10)\n",
    "print gpas\n",
    "\n",
    "# enumerate all possibilities\n",
    "combos = pd.DataFrame(cartesian([gres, gpas, [1, 2, 3, 4], [1.]]))\n",
    "# recreate the dummy variables\n",
    "combos.columns = ['gre', 'gpa', 'prestige', 'intercept']\n",
    "dummy_prestige = pd.get_dummies(combos['prestige'], prefix='prestige')\n",
    "dummy_prestige.columns = ['prestige_1', 'prestige_2', 'prestige_3', 'prestige_4']\n",
    "\n",
    "# keep only what we need for making predictions\n",
    "cols_to_keep = ['gre', 'gpa', 'prestige', 'intercept']\n",
    "combos = combos[cols_to_keep].join(dummy_prestige.ix[:, 'prestige_2':])\n",
    "\n",
    "# make predictions on the enumerated dataset\n",
    "combos['admit_pred'] = result.predict(combos[train_cols])\n",
    "\n",
    "combos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our predictions, let's make some plots to visualize the results. I created a small helper function called `isolate_and_plot()` which allows you to compare a given variable with the different prestige levels and the mean probability for that combination. To isolate prestige and the other variable I used a pivot_table which allows you to easily aggregate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isolate_and_plot(variable):\n",
    "    # isolate gre and class rank\n",
    "    grouped = pd.pivot_table(combos, values=['admit_pred'], index=[variable, 'prestige'],\n",
    "                            aggfunc=np.mean)\n",
    "    \n",
    "    # in case you're curious as to what this looks like\n",
    "    # print grouped.head()\n",
    "    #                      admit_pred\n",
    "    # gre        prestige            \n",
    "    # 220.000000 1           0.282462\n",
    "    #            2           0.169987\n",
    "    #            3           0.096544\n",
    "    #            4           0.079859\n",
    "    # 284.444444 1           0.311718\n",
    "    \n",
    "    # make a plot\n",
    "    colors = 'rbgyrbgy'\n",
    "    for col in combos.prestige.unique():\n",
    "        plt_data = grouped.ix[grouped.index.get_level_values(1)==col]\n",
    "        pl.plot(plt_data.index.get_level_values(0), plt_data['admit_pred'],\n",
    "                color=colors[int(col)])\n",
    "\n",
    "    pl.xlabel(variable)\n",
    "    pl.ylabel(\"P(admit=1)\")\n",
    "    pl.legend(['1', '2', '3', '4'], loc='upper left', title='Prestige')\n",
    "    pl.title(\"Prob(admit=1) isolating \" + variable + \" and presitge\")\n",
    "    pl.show()\n",
    "\n",
    "isolate_and_plot('gre')\n",
    "isolate_and_plot('gpa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plots shows how gre, gpa, and prestige affect the admission levels. You can see how the probability of admission gradually increases as gre and gpa increase and that the different prestige levels yield drastic probabilities of admission (particularly the most/least prestigious schools)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with `sklearn`\n",
    "Use the `sklearn.linear_model.LogisticRegression` class with a 70/30 train/test split to build and evaluate a model based on the dummified prestige (2, 3, 4) and the gre and gpa fields.  Score your model, how does it perform?  What other metrics can you examine?  What other ways could you cross-validate (hint: try LogisticRegressionCV)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Delay Prediction\n",
    "Our 2nd dataset for Logistic Regression practice will have us trying to predict which flights are going to be delayed.\n",
    "\n",
    "### Get the Data\n",
    "Download the [data](https://github.com/pburkard88/DS_BOS_06/blob/master/Data/Flight_Delays/FlightDelays.csv), load it in using `pandas.read_csv()` and inspect it with `head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "\n",
    "# Inspect with head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data\n",
    "The next step in predictive analytics is to explore our underlying data. Let’s do a few plots of our explantory variables to see how they look against Delayed Flights.\n",
    "\n",
    "Generate a histogram of the flight statuses.  Do this by calling `value_counts()` on the **Flight Status** column and then calling `plot()` with `kind='bar'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `value_counts()` function along with `shape` to return the percentage of records represented by each carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Percentage of records by each carrier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the following on carriers:  \n",
    "CO: Continental  \n",
    "DH: Atlantic Coast  \n",
    "DL: Delta  \n",
    "MQ: American Eagle  \n",
    "OH: Comair  \n",
    "RU: Continental Express  \n",
    "UA: United  \n",
    "US: US Airways  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating a `crosstab` and subsequent bar plot the same way we did above, this time with Flight Status and Day of the Week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice anything in particular?  Is there a way we could make this visualization more useful?\n",
    "\n",
    "Try the same thing for the Destination and Origin variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "This first transformation we’ll need to do is to convert the categorical variables into dummy variables.\n",
    "The four categorical variables of interests are: 1) Carrier 2) Destination (airport codes) 3) Origin (airport codes) 4) Day of the Week. For simplicity of model building, we’ll NOT use Day of the Month, because of the combinatorial explosion in number of dummy variables. The reader is free to do this as an exercise on his/her own. :)\n",
    "\n",
    "Use `pandas.get_dummies()` on those 4 categorical variables to create dummy variables for them and store them in a temporary dataframe `df_dummies`.  Generate a new dataframe `df` that contains only the numeric variables and dummified variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the weekday dummies\n",
    "\n",
    "# Set the weekday dummy column names to something better\n",
    "\n",
    "# Get the Carrier, Destination, and Origin dummies\n",
    "\n",
    "# Join the 2 dummies frames\n",
    "\n",
    "# Display the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Our Model\n",
    "Try fitting a model to our data using `sklearn`.  For now, just use the variables that we created as dummy variable predictors, and the **Flight Status** variable as our output variable.  You can use either the [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class with an explicit 70/30 train/test split or [`LogisticRegressionCV`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) with k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Train/test split\n",
    "\n",
    "# Create Model\n",
    "\n",
    "# Fit model\n",
    "\n",
    "# Score model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Our Results\n",
    "Print out the coefficients of your resulting model with a call to `coef`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.exp()` to determine the Odds Ratios for your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `zip()` and `sorted()` functions to generate a sorted list of the odds ratios alongside the respective table column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which fields are most/least indicative of flight delays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further\n",
    "- Use the `sklearn.metrics` package to return an ROC curve and other evaluation metrics for your model\n",
    "- Try incorporating the other data variables into your model and see if they improve things (be sure to standardize numeric values first!)\n",
    "- Repeat similar steps with `statsmodels`\n",
    "- For either exercise, is there some sort of threshold effect for any of the variables?  For instance, are there sharp changes in the way gpa affects admission (might represent a minimum allowable gpa or gre)?  If so, is it then possible to bucket any numeric variables to improve the model?  Perhaps explore some plots, or with pandas to see if you can find anything like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
