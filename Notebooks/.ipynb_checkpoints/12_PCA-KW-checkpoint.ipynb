{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA\n",
    "[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) is the process of reducing the number of random variables (features) under consideration while minimizing information loss in our data. The process of dimensionality reduction has a variety of applications and is used throughout the domain of data mining. Here we will explore the concepts behind [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis), and step through a couple of examples. \n",
    "- The first example is the canonical PCA example.\n",
    "- We will examine the handwritten digits dataset, specifically clustering by principal components.\n",
    "- We'll implement PCA on the Iris dataset.\n",
    "- More interestingly, we'll try out PCA for facial recognition on the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) dataset.\n",
    "\n",
    "### Learning Goals\n",
    "- Perform [***PCA with sklearn***](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- Use PCA as a precursor for clustering algorithms\n",
    "- Use PCA as a precursor for classification algorithms\n",
    "- Understand the output of ***PCA*** in terms of ***reduced features*** and ***explained variance***\n",
    "- Use PCA as dimensionality reduction for a face recognition (classification) problem\n",
    "\n",
    "### Datasets\n",
    "- Random Manufactured: a 2-dimensional randomly sampled gaussian\n",
    "- [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set): the familiar flower type classification dataset\n",
    "- [Handwritten Digits Dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html): `sklearn`'s built-in handwritten digits dataset\n",
    "- [Labeled Faces in the Wild Dataset](http://vis-www.cs.umass.edu/lfw/): Collection of headshot photographs of well-known people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets, random_projection \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Example - Random Data\n",
    "### Make a random dataset\n",
    "We'll generate a dataset from a randomly sampled 2-D Gaussian distribution of known mean and covariance matrix so we can demonstrate and visualize some of the principles (pun intended) of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify a covariance matrix with x-variance 2.9, y-variance 6.5, and x-y covariance -2.2\n",
    "cov = np.array([[2.9, -2.2], [-2.2, 6.5]])\n",
    "print 'cov: \\n{}'.format(cov)\n",
    "\n",
    "#Make a random set of data (drawing from a multivariate normal distribution)\n",
    "#Get 2-dimensional data with 500 rows (500X2 matrix)\n",
    "#The mean of the distribution is [1,2]\n",
    "#Covariance matrix of the distribution is cov\n",
    "norm_dist = np.random.multivariate_normal([1,2], cov, size=500)  \n",
    "print 'norm_dist: \\n{}...'.format(norm_dist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if norm_dist has approximately the expected sample mean and covariance\n",
    "print 'mean: \\n{}'.format(np.mean(norm_dist, axis=0))\n",
    "print 'covariance: \\n{}'.format(np.cov(norm_dist.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data\n",
    "Let's try plotting our data to take a look at it.  Based on the gaussian parameters we set, the data should be centered near [1,2], should vary more in the y direction than the x direction (from the y and x variances), and there should be a negative correlation between x and y (from the negative covariance coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a single figure to hold our plot\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "# Create the axes for our single plot\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "# Generate a scatter plot of the x and y values of our data in red\n",
    "ax.scatter(norm_dist[:,0], norm_dist[:,1], color='r')\n",
    "ax.axis('equal') # equal scaling on both axis;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a single figure to hold our plot\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "# Create the axes for our single plot\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "# Generate a scatter plot of the x and y values of our data in red\n",
    "print norm_dist[10,0]\n",
    "ax.scatter(norm_dist[10,0], norm_dist[10,1], color='r')\n",
    "ax.axis('equal') # equal scaling on both axis;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the data look as you expect?  Can you predict graphically where the first and second principal components might be (the directions along which the data varies most and 2nd most)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We would now like to analyze the directions in which the data varies most. \n",
    "\n",
    "##### For that, we place the point cloud in the center (0,0) and rotate a line through the data, such that the direction with most variance is parallel to the x-axis. Each succeding component in turn accounts for the highest variance possible that is orthoganal to existing components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let's perform principal component analysis (PCA) to project the data into fewer dimensions. In PCA, the projection is defined by principal components (eigenvectors), each of which can be viewed as a linear combination of the original features that corresponds to a dimension in the projection. The projection is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components. **Each principal component (eigenvector) is associated with an eigenvalue, which corresponds to the amount of the variance explained by that component.**\n",
    "\n",
    "**Dimensionality reduction is a one-way transformation that induces a loss of information.** We can try to minimize the loss of information while retaining the benefits of dimensionality reduction by trying to find the number of principal components needed to effectively represent the original dataset. This number can often be determined by the \"elbow\" or \"knee\" point, which is considered to be the natural break between the useful principal components (or dimensions) and residual noise. We can find the elbow point by computing PCA on our dataset and observing the number of principal components after which the amount of variance explained displays a natural break or drop-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components\n",
    "We'll use the [`sklearn.decomposition.PCA`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) object type to create an object that can perform PCA with the specified number of components.  Here we'll use `n_components=2` to examine the first 2 principal components, then we'll call `fit_transform()` on our data to transform our data into our new 2-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the PCA-generator so it will retain 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "# Transform the data into our 2-D PCA space\n",
    "norm_dist_pca = pca.fit_transform(norm_dist)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "# Create the axes for our single plot\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "# Generate a scatter plot of the x and y values of our data in red\n",
    "ax.scatter(norm_dist_pca[:,0], norm_dist_pca[:,1], color='r')\n",
    "ax.axis('equal') # equal scaling on both axis;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the resulting eigenvectors from our decomposition.  To do this we will call `pca.components_` which returns an array of 2 (for 2 principal components) 2D vectors (for the original 2D space) which represent the linear coefficients that transform the original space into the PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve and print the eigenvectors\n",
    "eigvecs = pca.components_\n",
    "print eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data\n",
    "Let's generate a few plots to demonstrate what the PCA is actually doing and provide some intuition.  We'll plot the 1st and 2nd principal components overlaying the original data in the original data space.  Then we'll plot the data in 1-D along both the 1st and 2nd principal components.  Finally we'll plot the data in the full transformed 2-D PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a figure to hold our plots\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "# Create axes for the first plot, the original data with the principal component axes overlaid\n",
    "ax = fig.add_subplot(2,2,1)\n",
    "# Generate a scatter plot of the original data in red\n",
    "ax.scatter(norm_dist[:,0], norm_dist[:,1], color='r', label='Original Data')\n",
    "# Generate x-values that range over the original dataset\n",
    "x = np.linspace(norm_dist.min(), norm_dist.max())\n",
    "# Store the sample mean of the original data\n",
    "x_mean = norm_dist.mean(0)[0]\n",
    "y_mean = norm_dist.mean(0)[1]\n",
    "# Generate equations for the 1st and 2nd principal component axes using point-slope form of a line going thru the mean\n",
    "y1 = eigvecs[0][1]*(x-x_mean)/eigvecs[0][0] + y_mean\n",
    "y2 = eigvecs[1][1]*(x-x_mean)/eigvecs[1][0] + y_mean\n",
    "# Plot the 1st and 2nd principal component axes over the original data\n",
    "# 1st component in blue, 2nd in green\n",
    "ax.plot(x, y1, color='b', label='1st Component')\n",
    "ax.plot(x, y2, color='g', label='2nd Component')\n",
    "ax.legend()\n",
    "ax.axis('equal') # equal scaling on both axis;\n",
    "ax.set_title('Original', size=24)\n",
    "\n",
    "# Create axes for the 2nd plot, which will show the points along only the 1st PCA axis\n",
    "ax = fig.add_subplot(2,2,2)\n",
    "# Plot the scatter plot along this axis\n",
    "ax.scatter(norm_dist_pca[:,0], [1 for x in norm_dist_pca[:,0]], color='b')\n",
    "ax.set_title('First Component', size=32)\n",
    "ax.set_yticklabels([])\n",
    "#ax.set_ylim()\n",
    "\n",
    "# Create axes for the 3rd plot, which will show the points along only the 2nd PCA axis\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "# Plot the scatter plot along this axis\n",
    "ax.scatter(norm_dist_pca[:,1], [1 for x in norm_dist_pca[:,1]], color='g')\n",
    "ax.set_title('Second Component', size=32)\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "# Create axes for the 4th plot, which will plot the data in the full PCA space\n",
    "ax = fig.add_subplot(2,2,4)\n",
    "#Fill this part in with the transformed data\n",
    "ax.scatter(norm_dist_pca[:,0], norm_dist_pca[:,1], color='y')\n",
    "ax.axis('equal') # equal scaling on both axis;\n",
    "ax.set_title('Transformed', size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What do you notice about these plots?  \n",
    "- Do the orientations of the PCA axes make sense to you?\n",
    "- How many total PCA components could there be with this dataset?\n",
    "\n",
    "Notice that the PCA axes are perpendicular (orthogonal), a requirement since the PCA eigenvectors form an orthogonal basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much of the dataset's variation is explained by each component?\n",
    "Let's use the `pca.explained_variance_ratio_` to print out how much variance is explained by each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What percentage of the variance in the original data is explained by the first/second Principal Components?\n",
    "- Does the total variance explained by the 2 combined make sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get introduced to PCA in `sklearn` with our friendly Iris dataset.  Load the dataset by a call to `sklearn.datasets.load_iris()`.  Store the results as `iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the iris data into iris\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the iris data as `X` and center it (subtract the mean) into a variable called `X_centered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get X\n",
    "X = iris.data\n",
    "print X\n",
    "# Center the data and store in X_centered\n",
    "mean = X.mean()\n",
    "X_centered = np.apply_along_axis(lambda x: x - mean, 0, X)\n",
    "X_centered = np.apply_along_axis(lambda x: x - mean, 1, X_centered)\n",
    "X_centered = np.apply_along_axis(lambda x: x - mean, 2, X_centered)\n",
    "X_centered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a PCA on `X_centered` with 2 principal components by first creating a `PCA` object with `n_components=2` and then calling `fit_transform()` on `X_centered`.  Store the results in `X_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform PCA with the first two components\n",
    "\n",
    "# Create the PCA-generator so it will retain 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "# Transform the data into our 2-D PCA space\n",
    "X_pca = pca.fit_transform(X_centered)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "# Create the axes for our single plot\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "# Generate a scatter plot of the x and y values of our data in red\n",
    "ax.scatter(X_pca[:,0], X_pca[:,1], color='r')\n",
    "ax.axis('equal') # equal scaling on both axis;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try plotting the situation in our 2-D PCA space using the below plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Useful plotting function\n",
    "from itertools import cycle\n",
    "\n",
    "def plot_PCA_2D(data, target, target_names):\n",
    "    colors = cycle('rgbcmykw')\n",
    "    target_ids = range(len(target_names))\n",
    "    plt.figure()\n",
    "    for i, c, label in zip(target_ids, colors, target_names):\n",
    "        plt.scatter(data[target == i, 0], data[target == i, 1],\n",
    "                   c=c, label=label)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add your transformed X and plot\n",
    "plot_PCA_2D(X_pca, target=iris.target, target_names=iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What observations can you make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Alternate PCA or Other Techniques\n",
    "Try using a `sklearn.decomposition.RandomizedPCA` instead to perform the same 3 steps (create PCA, fit PCA, plot results in 2D).  Feel free to try out any of the other dimensionality reduction techniques in `sklearn` as well if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1: If you finish try running the above with a randomized PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "rpca = RandomizedPCA()\n",
    "X_rpca = rpca.fit_transform(X_centered)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "# Create the axes for our single plot\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "# Generate a scatter plot of the x and y values of our data in red\n",
    "ax.scatter(X_rpca[:,0], X_rpca[:,1], color='r')\n",
    "ax.axis('equal') # equal scaling on both axis;\n",
    "\n",
    "plot_PCA_2D(X_rpca, target=iris.target, target_names=iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Handwritten Digits\n",
    "Now let's try using PCA to better explore our familiar handwritten digits dataset.\n",
    "\n",
    "### Load the Data\n",
    "Use a call to [`sklearn.datasets.load_digits()`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) to load the digits dataset into a variable called `digits` with only the digits 0-5 (`n_class=6`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the dataset with 6 classes (digits 0 through 5)\n",
    "digits = datasets.load_digits(n_class=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `pd.DataFrame()` and `pd.Series()` constructor to convert the feature and target data from `digits` into a dataframe `X` and a series `y` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the explanatory (or independent or feature) variables into a dataframe X\n",
    "X = pd.DataFrame(data=digits.data)\n",
    "# load the target (or dependent or class) variable into a series y\n",
    "y = pd.Series(data=digits.target)\n",
    "print y.head()\n",
    "print X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the shape of the data matrix `X` by a call to shape just to make sure the number of samples and features is right.  Store the number of samples in `n_samples` and the number of features in `n_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the number of rows (samples) and columns (features) with a call to shape\n",
    "n_samples = X.shape[0]\n",
    "print n_samples\n",
    "n_features = X.shape[1]\n",
    "print n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data\n",
    "Let's take a look at what some of our data actually looks like by printing out a number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_img_per_row = 20 # number of digits per row\n",
    "img = np.zeros((10*n_img_per_row, 10*n_img_per_row)) # generate a new 200x200 array filled with zeros\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix:ix+8, iy:iy+8] = X.ix[i*n_img_per_row + j].reshape((8, 8)) # set each 8x8 area of the img to the values of each row (reshaped from 1x64 to 8x8)\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=250) # define a figure, with size (width and height) and resolution\n",
    "#axes(frameon = 0) # remove the frame/border from the axes\n",
    "plt.imshow(img, cmap=plt.cm.binary) # show the image using a binary color map\n",
    "plt.xticks([]) # no x ticks\n",
    "plt.yticks([]) # no y ticks\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization\n",
    "Generally, PCA requires centering the data (i.e., subtracting the mean from each data point for each feature), because otherwise the first component may not truly describe the largest direction of variation in the data, but rather the mean of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# global centering\n",
    "X_centered = X - X.mean()\n",
    "\n",
    "# print again\n",
    "n_img_per_row = 20 # number of digits per row\n",
    "img = np.zeros((10*n_img_per_row, 10*n_img_per_row)) # generate a new 200x200 array filled with zeros\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix:ix+8, iy:iy+8] = X_centered.ix[i*n_img_per_row + j].reshape((8, 8)) # set each 8x8 area of the img to the values of each row (reshaped from 1x64 to 8x8)\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=250) # define a figure, with size (width and height) and resolution\n",
    "#axes(frameon = 0) # remove the frame/border from the axes\n",
    "plt.imshow(img, cmap=plt.cm.binary) # show the image using a binary color map\n",
    "plt.xticks([]) # no x ticks\n",
    "plt.yticks([]) # no y ticks\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA\n",
    "Now let's generate a PCA on the digits data.  \n",
    "- Create an object called `pca` that is a `sklearn.decomposition.PCA` object with `n_components=64` (we'll want to examine the different components later so keep all of the 64 possible principal components)\n",
    "- Use `pca.fit_transform()` on our centered digit data `X_centered` to transform the digit data into the PCA space.  Store the result in `X_pca`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create PCA-generator\n",
    "pca = PCA(n_components=64)\n",
    "\n",
    "# Transform X_centered to X_pca via a fit_transform\n",
    "X_pca = pca.fit_transform(X_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results\n",
    "Now that we have a PCA we can examine the different components.  Let's try to make a bar plot of the variance explained (proportion) for each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a single figure to hold our plot\n",
    "# fig = plt.figure(figsize=(5,5))\n",
    "# # Create the axes for our single plot\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "# # Generate a scatter plot of the x and y values of our data in red\n",
    "# ax.scatter(norm_dist[:,0], norm_dist[:,1], color='r')\n",
    "# ax.axis('equal') # equal scaling on both axis;\n",
    "\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a figure with figsize=(20,5)\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "# Generate axes via add_subplot\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "print type(ax)\n",
    "print pca.explained_variance_ratio_\n",
    "# Call ax.bar on the appropriate x and y to generate a bar graph of the explained variance for each component\n",
    "# for index in range(0,64):\n",
    "\n",
    "ax.bar(range(0,64),pca.explained_variance_ratio_)\n",
    "# x should be the numbers 1 to 64\n",
    "# y should be pca.explained_variance_ratio_\n",
    "\n",
    "ax.set_title(\"Explained variance\", size=32)\n",
    "ax.set_ylabel(\"Percent explained\", size=24)\n",
    "ax.set_xlabel(\"Principal Component\", size=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- What do you notice?\n",
    "- How many principal components do you think you might retain?\n",
    "\n",
    "Dimensionality reduction is a one-way transformation that induces a loss of information. We can try to minimize the loss of information while retaining the benefits of dimensionality reduction by trying to find the number of principal components needed to effectively represent the original dataset. This number can often be determined by the \"elbow\" or \"knee\" point, which is considered to be the natural break between the useful principal components (or dimensions) and residual noise. We can find the elbow point by computing PCA on our dataset and observing the number of principal components after which the amount of variance explained displays a natural break or drop-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Try Printing the first 10 \"Eigendigits\" in the same spirit as the \"Eigenfaces\" in the LFW Exercise below\n",
    "Don't do this part right now, but feel free to come back and try it after you've completed the Labeled Faces in the Wild facial recognition exercise later in this notebook.  Use similar techniques as the eigenfaces to print out the eigendigits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use components_ and reshape to get a 64x8x8 array of eigendigits\n",
    "\n",
    "# Generate a figure with figsize=(16,160)\n",
    "\n",
    "# For 0-9, print out the corresponding eigendigit in a subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets look at the first 2 components:\n",
    "Looking at only the first 2 components allows us to make actually visualize our dataset in 2-D to the best level that we can.  Use whatever method you like to determine how much variance is explained by the first 2 components in the digits PCA (**HINT:** use `explained_variance_ratio_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It's easy to represent 2 elements in a plot\n",
    "pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do in the next few steps is visualize how the different digits cluster in just 2 dimensions in our 2-D PCA space.  We'll start with a function `plot_embedding` which takes data and plots the digits in the 2-D space where the point is actually rendered as the digit itself.\n",
    "\n",
    "First we'll call this function on data randomly projected into 2-D from the original space and then on the PCA 2-D projection.  You should notice how well the PCA projection separates out the different classes even with just the first 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_embedding(X, title=None):\n",
    "    # min-max normalization\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=250)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.axis('off')\n",
    "    ax.patch.set_visible(False)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(digits.target[i]), color=plt.cm.Set1(y[i] / 10.), fontdict={'weight': 'bold', 'size': 12})\n",
    "\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.xlim([-0.1,1.1])\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random 2D Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random 2D projection using a random unitary matrix\n",
    "\n",
    "#print(\"Computing random projection\"),\n",
    "rp = random_projection.SparseRandomProjection(n_components=2, random_state=0)\n",
    "X_projected = rp.fit_transform(X)\n",
    "#print(\"done.\")\n",
    "\n",
    "# Plot random projection result\n",
    "plot_embedding(X_projected, \"Random Projection of the Digits Dataset\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(X_pca, \"2 components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA has no information about the classes, but provides insight into the distribution of different numbers in the parameter space\n",
    "- 0 and 4 tend to be more distinct then 1, 2, 5\n",
    "- Does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled Faces in the Wild Facial Recognition\n",
    "The dataset used in this example is a preprocessed excerpt of the\n",
    "\"Labeled Faces in the Wild\", aka LFW_:\n",
    "\n",
    "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
    "\n",
    ".. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "Expected results for the top 5 most represented people in the dataset::\n",
    "\n",
    "|                       | precision | recall | F1  | support |\n",
    "|-------------------    |-----------|--------|-----|---------|\n",
    "| **Gerhard_Schroeder** | .91       | .75    | .82 | 28      |\n",
    "| **Donald_Rumsfeld**   | .84       | .82    | .83 | 33      |\n",
    "| **Tony_Blair**        | .65       | .82    | .73 | 34      |\n",
    "| **Colin_Powell**      | .78       | .88    | .83 | 58      |\n",
    "| **George_W_Bush**     | .93       | .86    | .90 | 128     |\n",
    "| **avg / total**       | .86       | .84    | .85 | 282     |\n",
    "\n",
    "Our goal will be to build a classifier that successfully recognizes new images of the top 7 most frequently occurring people (these 5 plus 2 more) as the appropriate person.  We'll first run a PCA to reduce the dimensionality of the image space, and then use whatever classifier you like on the resulting PCA space to generate the classification predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import pylab as pl\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "Use the function [`fetch_lfw_people()`](http://scikit-learn.org/stable/datasets/labeled_faces.html) to load in the labeled faces data.  Make sure to set `data_home`, `min_faces_per_person=70` and `resize=0.4`.  Store the result as `lfw_people`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(data_home='/Users/keith/Dropbox/Data/DS_BOS_07/', min_faces_per_person=70, resize=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `shape` on `lfw_people.images` to get the number of images (store as `n_samples`), height (store as `h`), and width (store as `w`) of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288\n",
      "50\n",
      "37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1288, 50, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "print n_samples\n",
    "print h\n",
    "print w\n",
    "lfw_people.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store `lfw_people.data` into `X` and store the number of features in `X` a `n_features`.  What is `n_features`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 254.          254.          251.66667175 ...,   87.33333588\n",
      "    88.66666412   86.66666412]\n",
      " [  39.66666794   50.33333206   47.         ...,  117.66666412  115.\n",
      "   133.66667175]\n",
      " [  89.33333588  104.          126.         ...,  175.33332825\n",
      "   183.33332825  183.        ]\n",
      " ..., \n",
      " [  86.           80.33333588   75.         ...,   44.33333206\n",
      "    49.66666794   44.66666794]\n",
      " [  50.66666794   65.66666412   88.         ...,  197.          179.33332825\n",
      "   166.33332825]\n",
      " [  30.           27.           32.66666794 ...,   35.           35.33333206\n",
      "    61.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1850"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the data in X\n",
    "X = lfw_people.data\n",
    "print X\n",
    "\n",
    "# How many features do we have?  Print how many\n",
    "n_features = X.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store `lfw_people.target` into `y` and `lfw_people.target_names` into `target_names`.  Print each as well as the number of classes (store this as `n_classes`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6 3 ..., 5 3 5]\n",
      "['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n",
      " 'Gerhard Schroeder' 'Hugo Chavez' 'Tony Blair']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store targets in y\n",
    "y = lfw_people.target \n",
    "print y\n",
    "# Store target names in target_names\n",
    "target_names = lfw_people.target_names \n",
    "print target_names\n",
    "# Store number of classes in n_classes\n",
    "n_classes= len(target_names)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n"
     ]
    }
   ],
   "source": [
    "print \"Total dataset size:\"\n",
    "print \"n_samples: %d\" % n_samples\n",
    "print \"n_features: %d\" % n_features\n",
    "print \"n_classes: %d\" % n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Train/Test Set\n",
    "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the PCA\n",
    "Use a `RandomizedPCA` to construct a PCA (store as `pca`) which keeps `n_components=150` principal components and call `fit()` on `X_train` with this `pca` to generate the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 150 eigenfaces from 901 faces\n",
      "done in 0.146s\n"
     ]
    }
   ],
   "source": [
    "# Compute a PCA (eigenfaces) on the face dataset\n",
    "# Keep 150 principal components\n",
    "n_components = 150\n",
    "\n",
    "print \"Extracting the top %d eigenfaces from %d faces\" % (\n",
    "    n_components, X_train.shape[0])\n",
    "t0 = time()\n",
    "# Create a RandomizedPCA with n_components and fit it to X_train\n",
    "pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate `X_train_pca` by calling the pca transformation (`transform()`) on `X_train` and likewise generate `X_test_pca` by calling it on `X_test`.  After this we will have reduced space feature vectors in our PCA space for both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.021s\n"
     ]
    }
   ],
   "source": [
    "print \"Projecting the input data on the eigenfaces orthonormal basis\"\n",
    "t0 = time()\n",
    "\n",
    "# Transform X_train to X_train_pca\n",
    "X_train_pca = pca.transform(X_train)\n",
    "# Transform X_test to X_test_pca\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Classifier for Face Classification\n",
    "Now that we have a reduced vector space for our features of both the training and test sets, we can try to build a classifier against that space which will be able to accurately classify images into one of the 7 person classes seen earlier.  \n",
    "\n",
    "First let's try an RBF kernel SVM classifier, and let's try different parameters of `C` and `gamma` via `GridSearchCV`.  Generate a dict `param_grid` with possible values of `C` (1e3, 5e3, 1e4, 5e4, 1e5) and possible values of `gamma` (0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0],\n",
       " 'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a param_grid for GridSearchCV\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(901, 150)\n",
      "(901,)\n"
     ]
    }
   ],
   "source": [
    "print X_train_pca.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train your SVM Classifier via `GridSearchCV`:\n",
    "- Create a `GridSearchCV` called `clf` where the first parameter is a new `SVC` with an 'rbf' kernel and `class_weight='auto'`, and the second parameter is `param_grid`\n",
    "- Call `fit()` on `clf` with `X_train_pca` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "done in 13.045s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='auto', coef0=0.0, degree=3,\n",
      "  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,\n",
      "  random_state=None, shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Train a SVM classification model\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "# Create GridSearchCV\n",
    "clf = GridSearchCV(SVC(kernel='rbf',class_weight='auto'), param_grid)\n",
    "# Fit the model\n",
    "clf.fit(X_train_pca, y_train)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Our Model Success\n",
    "Now let's see how our model did:\n",
    "- Generate predictions `y_pred` by calling `predict()` against `X_test_pca`\n",
    "- Print out the `classification_report()` and `confusion_matrix()` for these predictions vs `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the people names on the testing set\n",
      "done in 0.047s\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Ariel Sharon       0.81      0.81      0.81        26\n",
      "     Colin Powell       0.80      0.88      0.84        82\n",
      "  Donald Rumsfeld       0.90      0.72      0.80        36\n",
      "    George W Bush       0.85      0.87      0.86       153\n",
      "Gerhard Schroeder       0.84      0.82      0.83        33\n",
      "      Hugo Chavez       0.82      0.70      0.76        20\n",
      "       Tony Blair       0.81      0.78      0.79        37\n",
      "\n",
      "      avg / total       0.83      0.83      0.83       387\n",
      "\n",
      "[[ 21   3   0   1   0   1   0]\n",
      " [  1  72   0   7   1   1   0]\n",
      " [  2   2  26   5   0   1   0]\n",
      " [  2  10   1 133   1   0   6]\n",
      " [  0   1   0   4  27   0   1]\n",
      " [  0   2   0   3   1  14   0]\n",
      " [  0   0   2   4   2   0  29]]\n"
     ]
    }
   ],
   "source": [
    "# Quantitative evaluation of the model quality on the test set\n",
    "# print X_test_pca\n",
    "print \"Predicting the people names on the testing set\"\n",
    "t0 = time()\n",
    "# Generate test predictions as y_pred\n",
    "y_pred = clf.best_estimator_.predict(X_test_pca)\n",
    "# print y_pred\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "# Print classification_report\n",
    "print classification_report(y_test, y_pred, target_names=target_names)\n",
    "# Print confusion_matrix\n",
    "print confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.39657058e-05,   1.47075882e-05,   1.47529897e-05, ...,\n",
       "          2.25045116e-05,   2.05630309e-05,   1.60484731e-05],\n",
       "       [ -3.62072311e-05,  -3.50887237e-05,  -3.64433571e-05, ...,\n",
       "          6.97372981e-05,   6.55619162e-05,   6.28721724e-05],\n",
       "       [  5.91214508e-05,   5.38357935e-05,   4.81823997e-05, ...,\n",
       "          1.21285204e-04,   1.07772611e-04,   9.60978651e-05],\n",
       "       ..., \n",
       "       [  8.90754113e-04,   1.04646963e-03,   9.72257108e-04, ...,\n",
       "          1.89365475e-03,  -5.65198563e-04,  -2.11445383e-03],\n",
       "       [  1.52864153e-03,  -2.48871888e-04,  -1.56260225e-03, ...,\n",
       "         -1.92120537e-03,   1.13555559e-03,   3.94519730e-03],\n",
       "       [  1.48986153e-03,   1.62399368e-03,   1.64334258e-03, ...,\n",
       "         -7.01691720e-04,   6.84738323e-04,  -1.24301264e-04]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did your model do?  Are you impressed with this performance?  Think you can do better?  If so, go ahead and try out some different models to see if you can improve.  Still, ~85% accuracy on this task without any sort of specific image processing feature detection going on is pretty impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Results: \"Eigenfaces\"\n",
    "The eigenvectors aka principal components in this example essentially represent what we'll call \"eigenfaces\".  These are basically ghostly faces making up the basis of faces in the PCA space (every face can be expressed as some linear combination of these eigenfaces).  Like usual, the eigenfaces are ordered from greatest to least distinguishing power.  \n",
    "\n",
    "The plot functions below will plot some of the first few eigenfaces, take a look and see if the results make sense to you.  Can you imagine why some results are happening?  Feel free to now try and implement a similar procedure for eigendigits back in the handwritten digits exercise from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the eigenfaces\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    pl.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        pl.subplot(n_row, n_col, i + 1)\n",
    "        pl.imshow(images[i].reshape((h, w)), cmap=pl.cm.gray)\n",
    "        pl.title(titles[i], size=12)\n",
    "        pl.xticks(())\n",
    "        pl.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' \"', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "# plot the gallery of the most significant eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
