{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing (LSI)  \n",
    "LSI is a specific application of dimensionality reduction to the problem of unstructured text processing.\n",
    "  \n",
    "At it's core LSI uses a simple Singular Value Decomposition (SVD) on a term-document matrix to perform the dimensionality reduction and map all documents and terms into one shared feature space.  This allows for pairwise conceptual comparison (by simple cosine similarity) of any documents or terms (term-term, term-document, document-document).  The higher the cosine similarity between two terms (or documents), the closer they are in terms of semantic or conceptual meaning.  \n",
    "  \n",
    "While the dimensionality reduction idea and term frequency vectors are not new, in it's 26 years of existence LSI has added considerable tradecraft making it (in this author's opinion) the state of the art technique for unstructured text analytics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an LSI Space\n",
    "What does it mean to \"build an LSI space\"?  This means you've completed the mapping that allows you to represent all documents and terms in the same k-dimensional vector space (where k is chosen).  Importantly, the space is built in such a way that the distance between any term/document vectors represents some measure of the conceptual (\"semantic\") similarity between them.  \n",
    "  \n",
    "The main steps to building an LSI space are:  \n",
    "<ol>\n",
    "<li>Procure the Data</li>\n",
    "<li>Build Term-Document Matrix</li>\n",
    "<li>Compute SVD</li>\n",
    "<li>Retain Term and Document Vectors</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procure the Data\n",
    "The input data to LSI is as simple as can be.  It just requires a set of documents containing unstructured text.  \n",
    "##### Download the Reuters Data\n",
    "Download the R52 sample of the ***Reuters 21578*** dataset from [here](http://www.csmining.org/index.php/r52-and-r8-of-reuters-21578.html) and store it locally (or you can just point `read_csv` directly to the download links.  \n",
    "  \n",
    "Use pandas to read in the dataset ***r52-train-stemmed*** dataset into a dataframe called `reuters_df_train` with the field names 'cat' and 'text' and read the ***r52-test-stemmed*** dataset into a dataframe called `reuters_df_test` with the same field names.  Make sure you note the value of the `sep` parameter in `read_csv()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cocoa</td>\n",
       "      <td>bahia cocoa review shower continu week bahia c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earn</td>\n",
       "      <td>champion product approv stock split champion p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acq</td>\n",
       "      <td>comput termin system cpml complet sale comput ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn</td>\n",
       "      <td>cobanco inc cbco year net shr ct dlr net asset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>intern inc qtr jan oper shr loss two ct profit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cat                                               text\n",
       "0  cocoa  bahia cocoa review shower continu week bahia c...\n",
       "1   earn  champion product approv stock split champion p...\n",
       "2    acq  comput termin system cpml complet sale comput ...\n",
       "3   earn  cobanco inc cbco year net shr ct dlr net asset...\n",
       "4   earn  intern inc qtr jan oper shr loss two ct profit..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import usual pandas and numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in r52 training set with stemming already done for you.\n",
    "reuters_df_train = pd.read_csv(\"http://www.csmining.org/tl_files/Project_Datasets/r8%20r52/r52-train-stemmed.txt\", sep ='\\t', names=['cat','text'])\n",
    "reuters_df_test = pd.read_csv(\"http://www.csmining.org/tl_files/Project_Datasets/r8%20r52/r52-train-stemmed.txt\", sep ='\\t', names=['cat','text'])\n",
    "\n",
    "# Use head to check out the first few rows of the dataset\n",
    "reuters_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bahia cocoa review shower continu week bahia cocoa zone allevi drought sinc earli januari and improv prospect for come temporao normal humid level not restor comissaria smith weekli review dry period mean temporao will late thi year arriv for week end februari bag kilo make cumul total for season mln stage last year that cocoa deliv earlier consign includ arriv figur comissaria smith doubt crop cocoa harvest practic come end total bahia crop estim mln bag and sale stand almost mln hundr thousand bag hand farmer middlemen export and processor doubt thi cocoa fit for export shipper experienc dificulti obtain bahia superior certif view lower qualiti recent week farmer sold good part cocoa held consign comissaria smith spot bean price rose cruzado per arroba kilo bean shipper reluct offer nearbi shipment and limit sale book for march shipment dlr per tonn port name crop sale light and open port june juli and dlr and and dlr york juli aug sept and dlr per tonn fob routin sale butter made march april sold and dlr april butter time york june juli and dlr aug sept dlr and and time york sept and oct dec dlr and time york dec comissaria smith destin covert currenc area uruguai and open port cake sale regist dlr for march april dlr for dlr for aug and time york dec for oct dec buyer argentina uruguai and convert currenc area liquor sale limit march april sell and dlr june juli dlr and time york juli aug sept dlr and time york sept and oct dec time york dec comissaria smith total bahia sale estim mln bag crop and mln bag crop final figur for period februari expect publish brazilian cocoa trade commiss carniv end middai februari reuter'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df_train.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the 20 Newsgroups Dataset\n",
    "Download the [20 Newsgroups Dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) with a call to [`sklearn.datasets.fetch_20newsgroups()`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups).  Download the 'train' subset first and store it in `ng_train`, then download the 'test' subset and store it in `ng_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import function\n",
    "\n",
    "# Download train\n",
    "\n",
    "# Download test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine 20 Newsgroups\n",
    "Take a look at the attributes of 20 Newsgroups and do the following:  \n",
    "- Create a `pd.Series` called `ng_train_text` from `ng_train.data`\n",
    "- Create a `pd.Series` called `ng_test_text` from `ng_test.data`\n",
    "- Create a `pd.Series` called `ng_train_cats` from `ng_train.target`\n",
    "- Create a `pd.Series` called `ng_test_cats` from `ng_test.target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Text\n",
    "\n",
    "# Training Cats\n",
    "\n",
    "# Test Text\n",
    "\n",
    "# Test Cats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAKE YOUR CHOICE\n",
    "Choose between the Reuters dataset for this exercise or the 20 newsgroups dataset at this step.  Whichever one you decide, store the training set text into a Series object called `docs` (make sure you retrieve the right column for Reuters if you choose it.  For the rest of the exercise, you will be working with the dataset you choose here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bahia cocoa review shower continu week bahia c...\n",
       "1    champion product approv stock split champion p...\n",
       "2    comput termin system cpml complet sale comput ...\n",
       "3    cobanco inc cbco year net shr ct dlr net asset...\n",
       "4    intern inc qtr jan oper shr loss two ct profit...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FOR REUTERS\n",
    "# We're not concerned with the category for now, so select out only the text column into a Series object 'docs'\n",
    "docs = reuters_df_train['text']\n",
    "## FOR 20NG\n",
    "# docs = ng_train_text\n",
    "# Use head to check out the first few rows of docs\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's load the nltk English stopwords list to ignore those in our analysis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "## Download various nltk corpora (used for stopwords here)\n",
    "# nltk.download()\n",
    "## Print all english stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Term-Document Matrix (TDM)\n",
    "In this step the goal is to encode all of the documents into a matrix where all of the unique terms in the dataset occur along the rows and the documents are the columns.  The values in each entry are some function of the term frequency for that particular term-document intersection.  \n",
    "\n",
    "There are a variety of different \"weightings\" that can be applied to the TDM entries, and this is one place where much of the tradecraft improvements of LSI occur.  The overall weighting scheme is generally TFIDF (Term Frequency Inverse Document Frequency) where the total weight is the product of the term frequency and inverse document frequency components:  \n",
    "\\begin{align} w_{ij} = wtf_{ij} \\cdot widf_i \\end{align}\n",
    "\n",
    "There are a handful of different term frequency (tf) weighting schemes ranging from binary (1/0 for whether a term occurred or not in the given document), to the actual frequency (count) or the log of the frequency.  For LSI, empirical results have led to the most common tf weighting of:  \n",
    "\\begin{align} wtf_{ij} = log(tf_{ij} + 1) \\end{align}  \n",
    "\n",
    "Similarly there are a handful of different global weighting (idf) schemes ranging from binary to logarithmic to an entropy function that has empirically been found best for LSI:  \n",
    "\\begin{align} widf_{i} = 1 + \\sum_j \\frac{p_{ij}log(p_{ij})}{log(n)} \\end{align}  \n",
    "\n",
    "In the above equation, $n$ is the number of documents, and $p_{ij}$ is the term frequency for a given document divided by the term's global frequency:  \n",
    "\\begin{align} p_{ij} = \\frac{tf_{ij}}{gtf_i} \\end{align}\n",
    "\n",
    "Given all this, the total weight for each entry is:\n",
    "\\begin{align} w_{ij} = wtf_{ij} \\cdot widf_i = log(tf_{ij} + 1) \\cdot \\left(1 + \\sum_j \\frac{p_{ij}log(p_{ij})}{log(n)}\\right) \\end{align}\n",
    "\n",
    "More details on all the different weightings can be found [here](http://en.wikipedia.org/wiki/Latent_semantic_indexing#Mathematics_of_LSI).\n",
    "\n",
    "##### Implement TDM Generator\n",
    "Let's implement a function that takes a Series of documents and generates the matrix with the LSI weightings from above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The way we're going to attack this is to build out the TDM matrix with the documents as rows and terms as columns\n",
    "and then we'll call the transpose operator to flip it to the representation we need for LSI.\n",
    "\n",
    "We need the following:\n",
    "    1.  Dictionary of word --> index to define vectors (index for each term)\n",
    "    2.  Dictionary of word --> total count to get the global (IDF)\n",
    "    3.  List of Dictionary of word --> document count for each document to get the local (TF) weighting\n",
    "'''\n",
    "\n",
    "# Implement a function that returns the 3 dictionaries that we need above\n",
    "def find_frequencies(docs):\n",
    "    term_indices = {} ## This is #1 above\n",
    "    currentIndex = 0 ## This is the counter to make sure we correctly populate the term indices in order\n",
    "    corpus_bag = {} ## This is #2 above\n",
    "    doc_bags = [] ## This is the collection for #3 above\n",
    "    for i, doc in docs.iteritems():\n",
    "        doc_bag = {} ## This is the dictionary of term frequencies for the doc we're currently examining, doc_bags stores a collection of these\n",
    "        ## TODO: Tokenize each document with nltk\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        ## TODO: For each token in the current document:\n",
    "        for token in tokens:\n",
    "            ## Optionally ignore stopword and continue\n",
    "#             if token in stopwords:\n",
    "#                 continue\n",
    "#                 ## If the word is new (not in term_indices): \n",
    "            if token not in term_indices:\n",
    "                ## add it to term_indices and give it the index value currentIndex, increment currentIndex\n",
    "                term_indices[token] = currentIndex\n",
    "                currentIndex += 1\n",
    "                corpus_bag[token] = 1\n",
    "                doc_bag[token] = 1\n",
    "\n",
    "            else:\n",
    "                ## If the word is not new:\n",
    "                ## increment the corpus_bag counter\n",
    "                corpus_bag[token] = corpus_bag[token] + 1        \n",
    "                if token not in doc_bag:\n",
    "                    doc_bag[token] = 1\n",
    "                else:\n",
    "                    doc_bag[token] += 1\n",
    "\n",
    "                    ## If the word is already in the doc_bag, increment that counter, else set it to 1\n",
    "        doc_bags.append(doc_bag)\n",
    "    return term_indices, corpus_bag, doc_bags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run find_frequencies on docs to return term_indices, corpus_bag, doc_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_indices, corpus_bag, doc_bags = find_frequencies(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the length of term_indices and make sure it is 16144.  Print out term_indices['cocoa'] and make sure it's an int 0-16143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print term_indices['cocoa'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the length of corpus_bag and make sure it is 16144.  Print out corpus_bag['cocoa'] and make sure it's 266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n"
     ]
    }
   ],
   "source": [
    "print corpus_bag['cocoa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the length of doc_bags[0] and make sure it's approximately the length of that document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "print len(doc_bags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Useful imports\n",
    "import math\n",
    "import scipy\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Implement a function that uses the corpus_bag and doc_bags found above to compute the global weighting (idf) term\n",
    "def compute_global_weight(corpus_bag, doc_bags):\n",
    "    global_weights = {} ## A dictionary of term --> global weight (the idf components) using entropy weighting\n",
    "    ## TODO: Define a variable logn which is the log base 2 of the number of documents in the set\n",
    "    logn = math.log(len(doc_bags), 2)\n",
    "    print logn\n",
    "    ## TODO: For each term:\n",
    "    for word in corpus_bag:\n",
    "    ## Start the global weight at 1\n",
    "        global_weights[word] = 1\n",
    "    ## Compute the global count from corpus_bag\n",
    "        global_count = corpus_bag[word]\n",
    "\n",
    "    ## For each doc_bag:\n",
    "        for doc_bag in doc_bags:\n",
    "            if word in doc_bag:\n",
    "        ## If the term is in it,\n",
    "                p_ij = (doc_bag[word] / (global_count * 1.0))\n",
    "                global_weights[word] = global_weights[word] + (p_ij * math.log(p_ij, 2) / logn)\n",
    "        #calculate p_ij and increase the global weight by p_ij * log(p_ij) / logn\n",
    "    ## Add this term's global weight to your global_weights dict\n",
    "    return global_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run compute_global_weight on corpus_bag and doc_bags to generate global_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6733090756\n"
     ]
    }
   ],
   "source": [
    "global_weights = compute_global_weight(corpus_bag, doc_bags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print global_weights['cocoa'] and make sure it's 0.594249632518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5942496325175995"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_weights['cocoa'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Finish the job with a function build_TDM that takes a Series 'docs' and outputs the TDM (a numpy matrix), make it also \n",
    "## return the term_indices and global weights as well\n",
    "def build_TDM(docs, fields=False,term_indices=[], corpus_bag=[], doc_bags=[],global_weights=[]):\n",
    "    ## TODO: Use your first 2 functions from above to populate the term_indices, corpus_bag, doc_bags and global_weights\n",
    "    if fields == False:\n",
    "        term_indices, corpus_bag, doc_bags = find_frequencies(docs)\n",
    "        global_weights = compute_global_weight(corpus_bag, doc_bags)\n",
    "    ## TODO: For each doc_bag, generate a doc_vec and add to doc_vecs (these are the \"vectors\" for each document with weighting)\n",
    "    doc_vecs = []\n",
    "    for doc_bag in doc_bags:\n",
    "        ## TODO: Initialize 'doc_vec' as a list of zeroes with 1 entry per unique term\n",
    "        doc_vec = [0]*len(corpus_bag)\n",
    "        ## TODO: For each term in the doc_bag, add the appropriate value into the appropriate place in the doc_vec\n",
    "        for term in doc_bag:\n",
    "            index = term_indices[term]\n",
    "            value = global_weights[term]*math.log(doc_bag[term] + 1.0, 2)\n",
    "            doc_vec[index] = value\n",
    "        doc_vecs.append(doc_vec)\n",
    "        ## NOTE: Need to take advantage of term_indices to get the right index, global_weights and doc_bag to get the value\n",
    "        ## TODO: Add the newest doc_vec to doc_vecs\n",
    "    ## TODO: Generate a numpy matrix from doc_vecs, and take it's transpose to give the TDM, return that\n",
    "    tdmatrix = np.matrix(doc_vecs).transpose()\n",
    "    return term_indices, global_weights, tdmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your build_TDM function on the docs you defined to generate a matrix called 'tdmatrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_indices, global_weights, tdmatrix = build_TDM(docs, True, term_indices, corpus_bag, doc_bags, global_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute SVD\n",
    "Once we have the TDM, the next step is the SVD.  The SVD performs the following decomposition:\n",
    "\\begin{align} X = T\\Sigma{D^T} \\end{align}\n",
    "Here $X$ is the TDM, which is a matrix of $m$ terms by $n$ documents.  $T$ is the resultant term space, it has a row for each of the $m$ terms and $r$ column features where $r$ is the <a href='http://en.wikipedia.org/wiki/Rank_(linear_algebra)'>rank</a> of $X$.  $\\Sigma$ is the square diagonal matrix of the $r$ [singular values](http://en.wikipedia.org/wiki/Singular_value) of $X$ in decreasing order.  The final matrix is the transpose of the resulting \"document space\" so it will be $r$ by $n$ where each column represents a document described by $r$ features.\n",
    "\n",
    "##### Try it out\n",
    "Use the linalg svd function with tdmatrix to perform the svd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Run the svd to yield the full term and document space matrices\n",
    "## WARNING: This is the computationally intensive step, it will take a long time, so make sure you've taken care of everything before\n",
    "## this as best as possible so you don't have to do it multiple times\n",
    "## Once this step is completed, essentially all the computational work is done, you have a trained LSI space!\n",
    "T,sigma,D_trans = linalg.svd(tdmatrix, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the Dimensionality: Rank Reduction\n",
    "At this point, we haven't really reduced the dimensionality of the problem (all terms and documents have $r$ features where $r$ is probably larger than we want to deal with).  So we make the following approximation:\n",
    "\\begin{align} X \\approx T_k\\Sigma_kD_k^T \\end{align}\n",
    "Here the first $k$ columns (where $k$ is a chosen parameter) have been selected from the $T$ matrix to yield the $m$ by $k$ matrix $T_k$.  The singular value matrix has been shrunk to $k$ by $k$ by removing any of the rows or columns greater than $k$.  The $D^T$ matrix has been truncated to $k$ by $n$ by selecting just the first $k$ rows of the matrix.\n",
    "\n",
    "The mathematics of the SVD tell us that this approximation is the best possible rank $k$ approximation to $X$ that we can possibly make.  Thus, by then using those matrices we have performed dimensionality reduction to $k$ dimensions.\n",
    "\n",
    "##### Try it Out\n",
    "Generate 3 matrices, T_k, D_trans_k, and sigma_inv_k by performing the rank-reduction approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16144, 100)\n",
      "(100, 6532)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "## Truncate the resulting matrices to dimension k (you select this dimension, higher values retain more information at complexity cost)\n",
    "k = 100\n",
    "m = T.shape[0]\n",
    "n = D_trans.shape[1]\n",
    "T_k = T[0:m, 0:k]\n",
    "print T_k.shape\n",
    "D_trans_k = D_trans[0:k, 0:n]\n",
    "print D_trans_k.shape\n",
    "sigma_inv = np.linalg.inv(linalg.diagsvd(sigma, n, n))\n",
    "sigma_inv_k = sigma_inv[0:k, 0:k]\n",
    "print sigma_inv_k.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Term and Document Vectors\n",
    "At this point, any of the terms can be plucked from the rows of the k-dimensional $T_k$ matrix and compared to one another for conceptual similarity.  Similarly, the same can be done for any document to document comparisons via the columns of the $D_k^T$ matrix.  \n",
    "\n",
    "#### Folding Document Vectors\n",
    "However, the one aspect that is missing is the ability to make comparisons between the two, aka we want to map the documents into the term space so that we can compare terms to documents.  The transformation that does this is:\n",
    "\\begin{align} D_k = X^TT_k\\Sigma_k^{-1} \\end{align}\n",
    "\n",
    "What this means, is that given any new document vector $d$ we can \"fold it in\" to the feature space by giving the vector the appropriate weightings (TFIDF) in the X space and then multiplying it by the matrices $T_k$ and $\\Sigma_k^{-1}$.\n",
    "\n",
    "##### Try it out:\n",
    "Implement a function fold_doc that takes a blob of text 'doc_text', term_indices, global_weights, T_k, and sigma_inv_k and returns a k dimension vector in the shared term-document space via document folding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function that folds a new document into an existing LSI space (space designated by global weightings, term indices, and T_k and sigma_inv_k)\n",
    "def fold_doc(doc_text, term_indices, global_weights, T_k, sigma_inv_k):\n",
    "    ## TODO: Fill out a doc_bag for the input doc_text (as previously done, tokenize, then build up a dictionary of word --> count)\n",
    "    doc_bag = {}\n",
    "    ## TODO: Generate an m-dimensional vector x_vec of zeroes that will eventually represent the document in the X space\n",
    "    ## TODO: Step through doc_bag and apply the appropriate local and global weightings and update the entry in x_vec\n",
    "    ## TODO: Conver x_vec to a numpy matrix x\n",
    "    # Perform the desired transformation to return folded vec\n",
    "    folded_vec = np.dot(np.dot(a, T_k), sigma_inv_k)\n",
    "    return folded_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "The way we will compare vectors in our LSI space is via [cosine similarity](http://en.wikipedia.org/wiki/Cosine_similarity).  The equation for it is as follows:\n",
    "\\begin{align} \\cos {(a,b)} = \\frac{a \\cdot b}{\\|{A}\\|\\|{B}\\|} \\end{align}\n",
    "\n",
    "Essentially, it is the normalized dot (scalar) product between 2 vectors.  This is the defacto standard for distance metric in LSI.\n",
    "\n",
    "##### Try it out\n",
    "Write a function cosine_sim that takes 2 vectors and returns their cosine similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out your function with the first few Reuters documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TODO: Fold each document into the space, and then compare them with cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Document Categorization with LSI\n",
    "Now that we've built our LSI space and we know how to fold in new documents into the space, we can try to perform the Reuters text categorization task.  Our approach will be a simple one, yet perform surprisingly well.  We will simply take the test set and for each document find the closest (by cosine similarity) document to it in the training set, then assign the test document to that training document's assigned category.   \n",
    "\n",
    "This is a simple manual KNN classifier, with k=1 and cosine similarity as a distance metric.  We can of course try different metrics and values of k if you like.\n",
    "\n",
    "##### Try it out\n",
    "Build a function classify_docs that takes a dataframe of documents with columns 'cat' and 'text' and our LSI Space components and does the following:\n",
    "<ul>\n",
    "<li>Prints the actual category and the predicted category for each document</li>\n",
    "<li>Prints the total docs tested</li>\n",
    "<li>Prints the total docs correct</li>\n",
    "<li>Prints the overall accuracy of prediction</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Fold in incoming documents into the space and categorize them\n",
    "def classify_docs(df_test, term_indices, global_weights, T_k, sigma_inv_k):\n",
    "    ## TODO: iterate through the rows of df_test using iterrows\n",
    "        ## TODO: Retrive the actual cat and test for each row\n",
    "        ## TODO: Fold the test document into the space to give it a vector\n",
    "        ## TODO: Compare the resultant vectors via cosine similarity and give each test document the category \n",
    "        ## of the training document closest to it.  Print out the right and predicted categories.  Keep track of right/wrong\n",
    "    ## TODO: Print out the summary results\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the R52 Stemmed Test set into a dataframe and try out the categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Considerations, Extensions, and Applications\n",
    "### Entity Extraction\n",
    "### Stopword Selection\n",
    "### Punctuation\n",
    "### Stemming\n",
    "### Alternative Weighting Schemes\n",
    "### Optimal Dimensionality Selection\n",
    "### Full Feature Utilization\n",
    "### Multilingual Corpora\n",
    "### Language Identification\n",
    "### Generic Item-Feature Problems\n",
    "### Majority Folding\n",
    "### Term Folding\n",
    "### Term Folding + Document Folding\n",
    "### Clustering\n",
    "### Recommendation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
